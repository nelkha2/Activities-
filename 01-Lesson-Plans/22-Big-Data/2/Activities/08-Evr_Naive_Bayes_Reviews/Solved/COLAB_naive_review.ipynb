{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "nteract": {
      "version": "0.11.2"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "naive_review.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdcugrJEI5cV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-eu.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.4-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRatGfeQI8_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5e8d2b90-00ce-4d3d-ca8a-c17c1db67a19"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.4-bin-hadoop2.7\"\n",
        "!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "!java -version"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_222\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_222-8u222-b10-1ubuntu1~18.04.1-b10)\n",
            "OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnFl0Oa-Ius2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "57417785-49d8-4ad6-b1ae-09391ae1bdfc"
      },
      "source": [
        "import findspark\n",
        "findspark.init(\"spark-2.4.4-bin-hadoop2.7\")# SPARK_HOME\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "\n",
        "# Read in data from S3 Buckets\n",
        "from pyspark import SparkFiles\n",
        "url =\"https://s3.amazonaws.com/dataviz-curriculum/day_2/yelp_reviews.csv\"\n",
        "spark.sparkContext.addFile(url)\n",
        "start_data = spark.read.csv(SparkFiles.get(\"yelp_reviews.csv\"), sep=\",\", header=True)\n",
        "\n",
        "# Show DataFrame\n",
        "start_data.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------------+\n",
            "|   class|                text|\n",
            "+--------+--------------------+\n",
            "|positive|Wow... Loved this...|\n",
            "|negative|  Crust is not good.|\n",
            "|negative|Not tasty and the...|\n",
            "|positive|Stopped by during...|\n",
            "|positive|The selection on ...|\n",
            "|negative|Now I am getting ...|\n",
            "|negative|Honeslty it didn'...|\n",
            "|negative|The potatoes were...|\n",
            "|positive|The fries were gr...|\n",
            "|positive|      A great touch.|\n",
            "|positive|Service was very ...|\n",
            "|negative|  Would not go back.|\n",
            "|negative|The cashier had n...|\n",
            "|positive|I tried the Cape ...|\n",
            "|negative|I was disgusted b...|\n",
            "|negative|I was shocked bec...|\n",
            "|positive| Highly recommended.|\n",
            "|negative|Waitress was a li...|\n",
            "|negative|This place is not...|\n",
            "|negative|did not like at all.|\n",
            "+--------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTWySjYmIutM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "04c72960-f841-4116-9dff-5a9b808483e2"
      },
      "source": [
        "\n",
        "from pyspark.sql.functions import length\n",
        "# Create a length column to be used as a future feature \n",
        "data_df = start_data.withColumn('length', length(start_data['text']))\n",
        "data_df.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------------+------+\n",
            "|   class|                text|length|\n",
            "+--------+--------------------+------+\n",
            "|positive|Wow... Loved this...|    24|\n",
            "|negative|  Crust is not good.|    18|\n",
            "|negative|Not tasty and the...|    41|\n",
            "|positive|Stopped by during...|    87|\n",
            "|positive|The selection on ...|    59|\n",
            "|negative|Now I am getting ...|    46|\n",
            "|negative|Honeslty it didn'...|    37|\n",
            "|negative|The potatoes were...|   111|\n",
            "|positive|The fries were gr...|    25|\n",
            "|positive|      A great touch.|    14|\n",
            "|positive|Service was very ...|    24|\n",
            "|negative|  Would not go back.|    18|\n",
            "|negative|The cashier had n...|    99|\n",
            "|positive|I tried the Cape ...|    59|\n",
            "|negative|I was disgusted b...|    62|\n",
            "|negative|I was shocked bec...|    50|\n",
            "|positive| Highly recommended.|    19|\n",
            "|negative|Waitress was a li...|    38|\n",
            "|negative|This place is not...|    51|\n",
            "|negative|did not like at all.|    20|\n",
            "+--------+--------------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn9PS_PhIutX",
        "colab_type": "text"
      },
      "source": [
        "### Feature Transformations\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZL2jl-cIutb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
        "# Create all the features to the data set\n",
        "pos_neg_to_num = StringIndexer(inputCol='class',outputCol='label')\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n",
        "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
        "hashingTF = HashingTF(inputCol=\"token_text\", outputCol='hash_token')\n",
        "idf = IDF(inputCol='hash_token', outputCol='idf_token')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ax9RJeBIuti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.linalg import Vector\n",
        "\n",
        "# Create feature vectors\n",
        "clean_up = VectorAssembler(inputCols=['idf_token', 'length'], outputCol='features')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_YPhRK4Iutq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Create a and run a data processing Pipeline\n",
        "from pyspark.ml import Pipeline\n",
        "data_prep_pipeline = Pipeline(stages=[pos_neg_to_num, tokenizer, stopremove, hashingTF, idf, clean_up])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDreIhV2Iuty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Fit and transform the pipeline\n",
        "cleaner = data_prep_pipeline.fit(data_df)\n",
        "cleaned = cleaner.transform(data_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6gKePL8Iut5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "a05dd032-0732-4235-8cf9-3c84baffa670"
      },
      "source": [
        "\n",
        "# Show label and resulting features\n",
        "cleaned.select(['label', 'features']).show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(262145,[33933,69...|\n",
            "|  1.0|(262145,[15889,13...|\n",
            "|  1.0|(262145,[25570,63...|\n",
            "|  0.0|(262145,[6286,272...|\n",
            "|  0.0|(262145,[6979,255...|\n",
            "|  1.0|(262145,[24417,24...|\n",
            "|  1.0|(262145,[12084,48...|\n",
            "|  1.0|(262145,[3645,963...|\n",
            "|  0.0|(262145,[53777,10...|\n",
            "|  0.0|(262145,[138356,2...|\n",
            "|  0.0|(262145,[24113,25...|\n",
            "|  1.0|(262145,[68867,13...|\n",
            "|  1.0|(262145,[24417,36...|\n",
            "|  0.0|(262145,[18098,24...|\n",
            "|  1.0|(262145,[24417,25...|\n",
            "|  1.0|(262145,[24417,25...|\n",
            "|  0.0|(262145,[31704,21...|\n",
            "|  1.0|(262145,[25570,27...|\n",
            "|  1.0|(262145,[12329,15...|\n",
            "|  1.0|(262145,[8287,139...|\n",
            "+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9HYHwQZIuuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "# Break data down into a training set and a testing set\n",
        "training, testing = cleaned.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Create a Naive Bayes model and fit training data\n",
        "nb = NaiveBayes()\n",
        "predictor = nb.fit(training)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqPITCjQIuuL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "4acc9a16-b0a9-49e6-bfdc-2f6899d7ebf5"
      },
      "source": [
        "\n",
        "# Tranform the model with the testing data\n",
        "test_results = predictor.transform(testing)\n",
        "test_results.show(5)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|   class|                text|length|label|          token_text|         stop_tokens|          hash_token|           idf_token|            features|       rawPrediction|         probability|prediction|\n",
            "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|negative|\"I don't know wha...|    85|  1.0|[\"i, don't, know,...|[\"i, know, big, d...|(262144,[8478,158...|(262144,[8478,158...|(262145,[8478,158...|[-870.84226085293...|[1.97138561757119...|       1.0|\n",
            "|negative|(It wasn't busy e...|    61|  1.0|[(it, wasn't, bus...|[(it, busy, eithe...|(262144,[329,2101...|(262144,[329,2101...|(262145,[329,2101...|[-752.96198557943...|[7.61598421122846...|       1.0|\n",
            "|negative|-Drinks took clos...|    58|  1.0|[-drinks, took, c...|[-drinks, took, c...|(262144,[98627,12...|(262144,[98627,12...|(262145,[98627,12...|[-713.63366092207...|[5.88497212517938...|       1.0|\n",
            "|negative|AVOID THIS ESTABL...|    25|  1.0|[avoid, this, est...|[avoid, establish...|(262144,[45245,10...|(262144,[45245,10...|(262145,[45245,10...|[-226.88037241225...|[6.25030642361132...|       1.0|\n",
            "|negative|After the disappo...|    61|  1.0|[after, the, disa...|[disappointing, d...|(262144,[16332,59...|(262144,[16332,59...|(262145,[16332,59...|[-578.36470557491...|[8.14576629626063...|       1.0|\n",
            "+--------+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBjx6SE7IuuR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "39f2f0b5-07c1-4e71-853b-9e150d5881ee"
      },
      "source": [
        "\n",
        "# Use the Class Evaluator for a cleaner description\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "acc_eval = MulticlassClassificationEvaluator()\n",
        "acc = acc_eval.evaluate(test_results)\n",
        "print(\"Accuracy of model at predicting reviews was: %f\" % acc)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of model at predicting reviews was: 0.741248\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}